{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import utils as u\n",
    "from models import Generator, Discriminator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIÁVEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOISE_DIM=100, N_CAMADAS=1, IMG_SIZE=8\n",
      "CPU times: user 1.51 s, sys: 191 ms, total: 1.7 s\n",
      "Wall time: 3.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NOISE_DIM = 100\n",
    "N_CAMADAS = 1  # ASSIM, O TAMANHO DA IMAGEM SERÁ N_CAMADAS*8. [1->8x8, 2->16x16, 3->32x32, 4->64x64, 5->128x128, 6->256x256]\n",
    "IMG_SIZE = 4 * (2**N_CAMADAS)\n",
    "IMG_CHANNELS = 3\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "MODELS_DIR = './models'\n",
    "IMGS_DIR = './imgs/celeba'\n",
    "TAXA_TREINAMENTO_DISCRIMINATOR = 5  # ou seja, o discriminator treina 5 vezes mais que o generator\n",
    "LAMBDA_GP = 10 # TAXA DO GRADIENT PENALTY\n",
    "\n",
    "def criar_transformer(img_size_):\n",
    "    return  transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size_, img_size_)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize(mean=(.5, .5, .5), std=(.5, .5, .5))\n",
    "    ])\n",
    "transformer = criar_transformer(img_size_=4)\n",
    "\n",
    "inv_transformer = transforms.Compose([\n",
    "    transforms.Normalize(mean=(-1., -1., -1.), std=(1., 1., 1.)),\n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "print (f'{NOISE_DIM=}, {N_CAMADAS=}, {IMG_SIZE=}')\n",
    "\n",
    "writer = SummaryWriter('./logs/pg-wgan-celeba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspeção de algumas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset)=202599\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.37 s, sys: 104 ms, total: 1.47 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = u.Custom_Dataset(IMGS_DIR, '*.jpg', transformer, inv_transformer)\n",
    "print (f'{len(dataset)=}')\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "grid_np = dataset.criar_grid_imagens_aleatorias(n_images=8, tipo='np')\n",
    "plt.figure(figsize=(20, 2))\n",
    "plt.imshow(grid_np)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos discriminator e generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise.shape=torch.Size([3, 100, 1, 1])\n",
      "output_generator.shape=torch.Size([3, 3, 8, 8])\n",
      "output_discriminator.shape=torch.Size([3, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(img_channels=IMG_CHANNELS, n_camadas=N_CAMADAS, cfl=128)\n",
    "generator = Generator(img_channels=IMG_CHANNELS, noise_dim=NOISE_DIM, n_camadas=N_CAMADAS, cfl=512)\n",
    "\n",
    "# Testando os modelos\n",
    "noise = u.get_noise(3, NOISE_DIM)\n",
    "output_generator = generator(noise)\n",
    "output_discriminator = discriminator(output_generator)\n",
    "print (f'{noise.shape=}')\n",
    "print (f'{output_generator.shape=}')\n",
    "print (f'{output_discriminator.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções úteis para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(model_d, real_imgs, fake_imgs, device_):\n",
    "\n",
    "    b_size, c, h, w = real_imgs.shape\n",
    "    alpha = torch.rand((b_size, 1, 1, 1)).repeat(1, c, h, w).to(device_)\n",
    "    # tenta criar um tensor (b,c,h,w) onde cada elemento b (de dim c,h,w) possui um número aleatório em todas as posições\n",
    "\n",
    "    interpolated_imgs = real_imgs * alpha + fake_imgs * (1-alpha)\n",
    "\n",
    "    # Cálculo score\n",
    "    mixed_score = model_d(interpolated_imgs)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_imgs,\n",
    "        outputs = mixed_score,\n",
    "        grad_outputs = torch.ones_like(mixed_score),\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    penalty = torch.mean((gradient_norm-1) ** 2)\n",
    "    \n",
    "    return penalty\n",
    "\n",
    "def imprimir_imagem_checkpoint():\n",
    "    random_noise = u.get_noise(8, NOISE_DIM, device)\n",
    "    random_img = generator(random_noise)\n",
    "    img_np = u.criar_grid(random_img, nrow=8, inv_transformer=inv_transformer, tipo='np')\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com N_CAMADAS=1 e tamanho das imagens IMG_SIZE=8\n",
      "Transformers criados.\n",
      "device=device(type='cuda', index=0)\n",
      "len(dataset)=202599, BATCH_SIZE=128, len(dataloader)=1583\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(img_channels=3, n_camadas=N_CAMADAS, cfl=128)\n",
    "generator = Generator(img_channels=3, noise_dim=NOISE_DIM, n_camadas=N_CAMADAS, cfl=512)\n",
    "noise = u.get_noise(1, NOISE_DIM)\n",
    "output_generator = generator(noise)\n",
    "IMG_SIZE = output_generator.shape[-1]\n",
    "\n",
    "print (f'Treinamento com {N_CAMADAS=} e tamanho das imagens {IMG_SIZE=}')\n",
    "transformer = criar_transformer(img_size_=IMG_SIZE)\n",
    "print (f'Transformers criados.')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (f'{device=}')\n",
    "\n",
    "discriminator.to(device)\n",
    "generator.to(device)\n",
    "\n",
    "optim_generator = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0., 0.9))\n",
    "optim_discriminator = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0., 0.9))\n",
    "\n",
    "dataset = u.Custom_Dataset(IMGS_DIR, pattern='*.jpg', transformer=transformer, inv_transformer=inv_transformer)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print (f'{len(dataset)=}, {BATCH_SIZE=}, {len(dataloader)=}')\n",
    "\n",
    "print (f'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 34/1583 [00:06<04:37,  5.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:510\u001b[0m, in \u001b[0;36m_getmp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 510\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m imprimir_imagem_checkpoint()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m real_imgs \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m     13\u001b[0m     real_imgs \u001b[38;5;241m=\u001b[39m real_imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     b_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(real_imgs)\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/tqdm/std.py:1180\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1177\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/torch/utils/data/dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 517\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    556\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    559\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/PG-GAN-CELEBA/utils.py:26\u001b[0m, in \u001b[0;36mCustom_Dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlista\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/PIL/Image.py:2994\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2991\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   2992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2994\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m init():\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/PIL/Image.py:2980\u001b[0m, in \u001b[0;36mopen.<locals>._open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[1;32m   2979\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 2980\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2981\u001b[0m     _decompression_bomb_check(im\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m   2982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:801\u001b[0m, in \u001b[0;36mjpeg_factory\u001b[0;34m(fp, filename)\u001b[0m\n\u001b[1;32m    799\u001b[0m im \u001b[38;5;241m=\u001b[39m JpegImageFile(fp, filename)\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 801\u001b[0m     mpheader \u001b[38;5;241m=\u001b[39m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getmp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mpheader[\u001b[38;5;241m45057\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;66;03m# It's actually an MPO\u001b[39;00m\n\u001b[1;32m    804\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMpoImagePlugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MpoImageFile\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:479\u001b[0m, in \u001b[0;36mJpegImageFile._getmp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getmp\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getmp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/principal/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:510\u001b[0m, in \u001b[0;36m_getmp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getmp\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;66;03m# Extract MP information.  This method was inspired by the \"highly\u001b[39;00m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# experimental\" _getexif version that's been in use for years now,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# The MP record essentially consists of a TIFF file embedded in a JPEG\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;66;03m# application marker.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 510\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    imprimir_imagem_checkpoint()\n",
    "    \n",
    "    print (f'{epoch=}')\n",
    "\n",
    "    for real_imgs in tqdm(dataloader):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        b_size = len(real_imgs)\n",
    "\n",
    "        # Treinando o discriminator\n",
    "        for _ in range(TAXA_TREINAMENTO_DISCRIMINATOR):\n",
    "            noise = u.get_noise(b_size, NOISE_DIM, device)\n",
    "            fake_imgs = generator(noise)\n",
    "\n",
    "            output_real = discriminator(real_imgs)\n",
    "            output_fake = discriminator(fake_imgs)\n",
    "\n",
    "            # Cálculo do Gradient-penalty\n",
    "            gp = gradient_penalty(discriminator, real_imgs, fake_imgs, device)\n",
    "            loss_discriminator = -(torch.mean(output_real.view(-1)) - torch.mean(output_fake.view(-1))) + LAMBDA_GP*gp\n",
    "            discriminator.zero_grad()\n",
    "            loss_discriminator.backward(retain_graph=True)\n",
    "            optim_discriminator.step()\n",
    "        \n",
    "        # Treinando o generator\n",
    "        output_fake_for_generator = discriminator(fake_imgs)\n",
    "        loss_generator = -torch.mean(output_fake_for_generator.view(-1))\n",
    "        generator.zero_grad()\n",
    "        loss_generator.backward()\n",
    "        optim_generator.step()\n",
    "\n",
    "    # Levando as variáveis para o tensorboard\n",
    "    writer.add_scalar('loss_discriminator', loss_discriminator.item(), epoch)\n",
    "    writer.add_scalar('loss_generator', loss_generator.item(), epoch)\n",
    "    \n",
    "    if (epoch % 5 == 0):\n",
    "        discriminator.save(epoch)\n",
    "        generator.save(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa39838d5afd7d94b7544cb5e5351cace91d1e0eb74b6451fdb6f11f3a068bed"
  },
  "kernelspec": {
   "display_name": "principal:Python",
   "language": "python",
   "name": "conda-env-principal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
