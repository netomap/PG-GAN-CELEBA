{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import utils as u\n",
    "from models import Generator, Discriminator\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.tensorboard.writer import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIÁVEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOISE_DIM=100, N_CAMADAS=1, IMG_SIZE=8\n",
      "CPU times: user 1.43 s, sys: 209 ms, total: 1.64 s\n",
      "Wall time: 3.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NOISE_DIM = 100\n",
    "N_CAMADAS = 1  # ASSIM, O TAMANHO DA IMAGEM SERÁ N_CAMADAS*8. [1->8x8, 2->16x16, 3->32x32, 4->64x64, 5->128x128, 6->256x256]\n",
    "IMG_SIZE = 4 * (2**N_CAMADAS)\n",
    "IMG_CHANNELS = 3\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 128\n",
    "MODELS_DIR = './models'\n",
    "IMGS_DIR = './imgs/celeba'\n",
    "TAXA_TREINAMENTO_DISCRIMINATOR = 5  # ou seja, o discriminator treina 5 vezes mais que o generator\n",
    "LAMBDA_GP = 10 # TAXA DO GRADIENT PENALTY\n",
    "\n",
    "def criar_transformer(img_size_):\n",
    "    return  transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((img_size_, img_size_)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize(mean=(.5, .5, .5), std=(.5, .5, .5))\n",
    "    ])\n",
    "transformer = criar_transformer(img_size_=4)\n",
    "\n",
    "inv_transformer = transforms.Compose([\n",
    "    transforms.Normalize(mean=(-1., -1., -1.), std=(1., 1., 1.)),\n",
    "    transforms.ToPILImage(),\n",
    "])\n",
    "\n",
    "print (f'{NOISE_DIM=}, {N_CAMADAS=}, {IMG_SIZE=}')\n",
    "\n",
    "writer = SummaryWriter('./logs/pg-wgan-celeba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspeção de algumas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dataset)=202599\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAB7CAYAAAA8LsLzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAGH0lEQVR4nO3dS4iVdRjH8ec4zpgX1IhULLKMpLKoRDdToBEohIaNgUbqytRq04WgIhJKjYLauTAyiYoky0gypCgpNbQEL2QyZlhk4q3yUjpqelq0nllIPPHI57P9L37vHGZezvd9F9NoNpsBAABAPb3+7wsAAADgwgg6AACAogQdAABAUYIOAACgKEEHAABQlKADAAAoqndPh7u2bEj7nwbzZk7Pmop7OjrStibPfiRtKyLi5ptuTNuaOTvvc1z7ybq0rQMHj6Zttd8xKW0rIuK7HRvTtloaec+LXn/zo7StSRPHpG1dzJrNc2lbm77Znbb1+PSJaVvPv7Eibat3S/+0rYiIAXvy7vntd41N22r0aknbevbp5WlbL616N23rX+fTlibMWJy29dlbT6ZtnTmdNhV/rsv7uSIiDqzfl7a16OudaVvvbehsdHfmDR0AAEBRgg4AAKAoQQcAAFCUoAMAAChK0AEAABQl6AAAAIoSdAAAAEUJOgAAgKIEHQAAQFGCDgAAoChBBwAAUJSgAwAAKErQAQAAFCXoAAAAihJ0AAAARQk6AACAogQdAABAUYIOAACgKEEHAABQlKADAAAoStABAAAUJegAAACKEnQAAABFCToAAICiBB0AAEBRgg4AAKCo3j0dDvr7UNZ1xIcrV6Rtbf1qZdrWzs7v07YiIh6a/0Da1uDmb2lb024amra15ODxtK2f9+9N24qI+HLJi2lbx38/nLb1zqer07Ym3D4ibevwxtfStiIihrcNT9s6P/6+tK1XH+5I2zpxJO/+cdmh3Wlbp0aMTduKiBh9/RVpW61796Rt/bRqU9rW+Yv4kf3kGfPTtvZt35W2ddtdO9K2tqxZlra144O1aVsREXe//UPaVr+2trStnlzEf+4AAAAXN0EHAABQlKADAAAoStABAAAUJegAAACKEnQAAABFCToAAICiBB0AAEBRgg4AAKAoQQcAAFCUoAMAAChK0AEAABQl6AAAAIoSdAAAAEUJOgAAgKIEHQAAQFGCDgAAoChBBwAAUJSgAwAAKErQAQAAFCXoAAAAihJ0AAAARQk6AACAogQdAABAUYIOAACgKEEHAABQVO+eDo8cPJR1HdH4dUve1h9pU/HFxwvyxiKidfBVaVsT2u9I21q372Ta1pChB9K2nnvhpbStiIi2v39J2+rXp0/a1v3Tp6Zt7Vw6N23rhlFT0rYiIs605W316mpJ2xp++aC0rTcWvpK2dbTrWNrW5s5taVsREbffOjBt6+SR/WlbP5/+M22r0Zr3Gb684Km0rYiIjqF53wmGzbk3bevbc33Tts6dP5O2NX759rStiIhTd85L29q9fnPaVk+8oQMAAChK0AEAABQl6AAAAIoSdAAAAEUJOgAAgKIEHQAAQFGCDgAAoChBBwAAUJSgAwAAKErQAQAAFCXoAAAAihJ0AAAARQk6AACAogQdAABAUYIOAACgKEEHAABQlKADAAAoStABAAAUJegAAACKEnQAAABFCToAAICiBB0AAEBRgg4AAKAoQQcAAFCUoAMAAChK0AEAABQl6AAAAIpqNJvNbg+XzG3v/vA/ds3V12ZNRUvX6bStUePa07YiIsbNWZy2Nf66K9O2zja60rbWbNmbtjXs8uFpWxERQ/q3pG2d+OtY2ta0WY+mbU0ZujFta8AlY9K2IiIeW7AwbWvN1s60rZEjb0nbGj2wb9rWwcRHsgNbG3ljEbF6Wd7v4rYd+9O2Zr+4Im1rcGve/f6ZRU+kbUVEzJrzYNrW3KmT0rZ+7HV12tbn7y9N24rIvX+0NAambZ09m/edsdFyabcfpDd0AAAARQk6AACAogQdAABAUYIOAACgKEEHAABQlKADAAAoStABAAAUJegAAACKEnQAAABFCToAAICiBB0AAEBRgg4AAKAoQQcAAFCUoAMAAChK0AEAABQl6AAAAIoSdAAAAEUJOgAAgKIEHQAAQFGCDgAAoChBBwAAUJSgAwAAKErQAQAAFCXoAAAAihJ0AAAARQk6AACAohrNZvP/vgYAAAAugDd0AAAARQk6AACAogQdAABAUYIOAACgKEEHAABQlKADAAAo6h9tNMUV7tA62AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 s, sys: 90.2 ms, total: 1.25 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset = u.Custom_Dataset(IMGS_DIR, '*.jpg', transformer, inv_transformer)\n",
    "print (f'{len(dataset)=}')\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "grid_np = dataset.criar_grid_imagens_aleatorias(n_images=8, tipo='np')\n",
    "plt.figure(figsize=(20, 2))\n",
    "plt.imshow(grid_np)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos discriminator e generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(img_channels=IMG_CHANNELS, n_camadas=N_CAMADAS, cfl=128)\n",
    "generator = Generator(img_channels=IMG_CHANNELS, noise_dim=NOISE_DIM, n_camadas=N_CAMADAS, cfl=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise.shape=torch.Size([3, 100, 1, 1])\n",
      "output_generator.shape=torch.Size([3, 3, 8, 8])\n",
      "output_discriminator.shape=torch.Size([3, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Testando os modelos\n",
    "noise = u.get_noise(3, NOISE_DIM)\n",
    "output_generator = generator(noise)\n",
    "output_discriminator = discriminator(output_generator)\n",
    "print (f'{noise.shape=}')\n",
    "print (f'{output_generator.shape=}')\n",
    "print (f'{output_discriminator.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções úteis para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(model_d, real_imgs, fake_imgs, device_):\n",
    "\n",
    "    b_size, c, h, w = real_imgs.shape\n",
    "    alpha = torch.rand((b_size, 1, 1, 1)).repeat(1, c, h, w).to(device_)\n",
    "    # tenta criar um tensor (b,c,h,w) onde cada elemento b (de dim c,h,w) possui um número aleatório em todas as posições\n",
    "\n",
    "    interpolated_imgs = real_imgs * alpha + fake_imgs * (1-alpha)\n",
    "\n",
    "    # Cálculo score\n",
    "    mixed_score = model_d(interpolated_imgs)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs = interpolated_imgs,\n",
    "        outputs = mixed_score,\n",
    "        grad_outputs = torch.ones_like(mixed_score),\n",
    "        create_graph = True,\n",
    "        retain_graph = True\n",
    "    )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    penalty = torch.mean((gradient_norm-1) ** 2)\n",
    "    \n",
    "    return penalty\n",
    "\n",
    "def imprimir_imagem_checkpoint():\n",
    "    random_noise = u.get_noise(8, NOISE_DIM, device)\n",
    "    random_img = generator(random_noise)\n",
    "    img_np = u.criar_grid(random_img, nrow=8, inv_transformer=inv_transformer, tipo='np')\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento com N_CAMADAS=1 e tamanho das imagens IMG_SIZE=8\n",
      "Transformers criados.\n",
      "device=device(type='cuda', index=0)\n",
      "len(dataset)=202599, BATCH_SIZE=128, len(dataloader)=1583\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(img_channels=3, n_camadas=N_CAMADAS, cfl=128)\n",
    "generator = Generator(img_channels=3, noise_dim=NOISE_DIM, n_camadas=N_CAMADAS, cfl=512)\n",
    "noise = u.get_noise(1, NOISE_DIM)\n",
    "output_generator = generator(noise)\n",
    "IMG_SIZE = output_generator.shape[-1]\n",
    "\n",
    "print (f'Treinamento com {N_CAMADAS=} e tamanho das imagens {IMG_SIZE=}')\n",
    "transformer = criar_transformer(img_size_=IMG_SIZE)\n",
    "print (f'Transformers criados.')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (f'{device=}')\n",
    "\n",
    "discriminator.to(device)\n",
    "generator.to(device)\n",
    "\n",
    "optim_generator = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0., 0.9))\n",
    "optim_discriminator = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0., 0.9))\n",
    "\n",
    "dataset = u.Custom_Dataset(IMGS_DIR, pattern='*.jpg', transformer=transformer, inv_transformer=inv_transformer)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print (f'{len(dataset)=}, {BATCH_SIZE=}, {len(dataloader)=}')\n",
    "\n",
    "print (f'ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1583 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m b_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(real_imgs)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Treinando o discriminator\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m TAXA_TREINAMENTO_DISCRIMINATOR:\n\u001b[1;32m     18\u001b[0m     noise \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mget_noise(b_size, NOISE_DIM, device)\n\u001b[1;32m     19\u001b[0m     fake_imgs \u001b[38;5;241m=\u001b[39m generator(noise)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(1_000):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    imprimir_imagem_checkpoint()\n",
    "    \n",
    "    print (f'{epoch=}')\n",
    "\n",
    "    for real_imgs in tqdm(dataloader):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        b_size = len(real_imgs)\n",
    "\n",
    "        # Treinando o discriminator\n",
    "        for _ in range(TAXA_TREINAMENTO_DISCRIMINATOR):\n",
    "            noise = u.get_noise(b_size, NOISE_DIM, device)\n",
    "            fake_imgs = generator(noise)\n",
    "\n",
    "            output_real = discriminator(real_imgs)\n",
    "            output_fake = discriminator(fake_imgs)\n",
    "\n",
    "            # Cálculo do Gradient-penalty\n",
    "            gp = gradient_penalty(discriminator, real_imgs, fake_imgs, device)\n",
    "            loss_discriminator = -(torch.mean(output_real.view(-1)) - torch.mean(output_fake.view(-1))) + LAMBDA_GP*gp\n",
    "            discriminator.zero_grad()\n",
    "            loss_discriminator.backward(retain_graph=True)\n",
    "            optim_discriminator.step()\n",
    "        \n",
    "        # Treinando o generator\n",
    "        output_fake_for_generator = discriminator(fake_imgs)\n",
    "        loss_generator = -torch.mean(output_fake_for_generator.view(-1))\n",
    "        generator.zero_grad()\n",
    "        loss_generator.backward()\n",
    "        optim_generator.step()\n",
    "\n",
    "    # Levando as variáveis para o tensorboard\n",
    "    writer.add_scalar('loss_discriminator', loss_discriminator.item(), epoch)\n",
    "    writer.add_scalar('loss_generator', loss_generator.item(), epoch)\n",
    "    \n",
    "    if (epoch % 100 == 0):\n",
    "        discriminator.save()\n",
    "        generator.save()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa39838d5afd7d94b7544cb5e5351cace91d1e0eb74b6451fdb6f11f3a068bed"
  },
  "kernelspec": {
   "display_name": "principal:Python",
   "language": "python",
   "name": "conda-env-principal-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
